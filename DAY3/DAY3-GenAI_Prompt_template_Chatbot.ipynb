{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae8d42-a30e-4cf9-bf03-620f6f1dd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recap\n",
    "------\n",
    "[Documents load ] -> [ chunk ] -> [ embedding ] -> [vector DB - index]\n",
    "                         |               |          |-> Chroma.from_documents(final_docs,embedding)\n",
    "load env variables      final_docs       |-> from langchain_openai import OpenAIEmbeddings\n",
    ".env file                                    embedding=OpenAIEmbeddings(model='<modelName>')\n",
    "---------                                    embedding.embed_query('some text')\n",
    "     |->OPENAI_API_KEY=\"\"                   \n",
    "     |->LANG_CHAIN\n",
    "     |->project\n",
    "\n",
    "import os\n",
    "os.environ # To see the existing env variables\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] ->KeyError\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load all the env variables - reading from .env file\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0a374-c444-4afe-b624-4ea07804e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt template\n",
    "|\n",
    "User Interface \n",
    "|\n",
    "Appln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef22847e-aee0-4e02-b22e-dc0e04522bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f832088b-80f1-472c-befd-d6d805a173a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.environ['HF_TOKEN']) ->KeyError\n",
    "from dotenv import load_dotenv\n",
    "#print(os.getenv('HF_TOKEN')) ->empty\n",
    "load_dotenv()\n",
    "#print(os.getenv('HF_TOKEN'))  ->Token\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc41e74-c391-4034-8f5b-2e493aba550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN']=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0f4b87-9787-45fe-bac1-e94727784234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n",
      "[-0.03447727486491203, 0.03102312609553337, 0.006734980270266533, 0.026108933612704277, -0.03936205804347992, -0.16030246019363403, 0.06692394614219666, -0.006441438104957342, -0.047450482845306396, 0.014758863486349583, 0.07087534666061401, 0.05552757531404495, 0.019193356856703758, -0.02625126577913761, -0.01010954286903143, -0.026940442621707916, 0.022307462990283966, -0.02222665585577488, -0.14969263970851898, -0.017493024468421936, 0.007676282897591591, 0.054352231323719025, 0.0032544038258492947, 0.03172588348388672, -0.08462139964103699, -0.029405992478132248, 0.051595550030469894, 0.048124078661203384, -0.003314835485070944, -0.05827915295958519, 0.04196925833821297, 0.022210702300071716, 0.1281888633966446, -0.022338951006531715, -0.011656239628791809, 0.06292837113142014, -0.03287634998559952, -0.09122604131698608, -0.03117534890770912, 0.052699536085128784, 0.04703483358025551, -0.08420310169458389, -0.030056182295084, -0.020744839683175087, 0.009517822414636612, -0.003721762914210558, 0.00734331738203764, 0.03932439535856247, 0.09327399730682373, -0.0037885578349232674, -0.05274209380149841, -0.05805816873908043, -0.006864397320896387, 0.005283216945827007, 0.0828929990530014, 0.019362706691026688, 0.006284490693360567, -0.010330768302083015, 0.009032364934682846, -0.037683695554733276, -0.045206066220998764, 0.024016287177801132, -0.006944162305444479, 0.013491649180650711, 0.10005491971969604, -0.07168387621641159, -0.021695099771022797, 0.03161845728754997, -0.05163457617163658, -0.0822477638721466, -0.06569331884384155, -0.009895353578031063, 0.005816371645778418, 0.07355457544326782, -0.03405033051967621, 0.02488613687455654, 0.014488051645457745, 0.02645733952522278, 0.00965669471770525, 0.03021721914410591, 0.05280400812625885, -0.07535992562770844, 0.009897175244987011, 0.02983679808676243, 0.01755562797188759, 0.02309201844036579, 0.0019338484853506088, 0.0014002117095515132, -0.047175947576761246, -0.01119430921971798, -0.11420140415430069, -0.019811974838376045, 0.04026620835065842, 0.002193015068769455, -0.07979219406843185, -0.025382323190569878, 0.09448298066854477, -0.028981072828173637, -0.1450025588274002, 0.23097744584083557, 0.027731169015169144, 0.03211147338151932, 0.03106500953435898, 0.04283282533288002, 0.06423778086900711, 0.03216315060853958, -0.004876742139458656, 0.05569940060377121, -0.03753245621919632, -0.021505557000637054, -0.02834269404411316, -0.028846969828009605, 0.038353074342012405, -0.017468687146902084, 0.052485279738903046, -0.07487601786851883, -0.03125973790884018, 0.021841581910848618, -0.039895690977573395, -0.008587119169533253, 0.026956595480442047, -0.04849553480744362, 0.011469870805740356, 0.029618198052048683, -0.02057221718132496, 0.013103905133903027, 0.02883351780474186, -3.194198350138249e-33, 0.06478210538625717, -0.01813018135726452, 0.05178992450237274, 0.12198279052972794, 0.028780171647667885, 0.008721987716853619, -0.07052117586135864, -0.016907278448343277, 0.04073971137404442, 0.042116183787584305, 0.025447247549891472, 0.03574629873037338, -0.049144718796014786, 0.002128997351974249, -0.01554661151021719, 0.05073055252432823, -0.04818534478545189, 0.035880617797374725, -0.004067136440426111, 0.10172470659017563, -0.05597003176808357, -0.010681075975298882, 0.011235758662223816, 0.09068655222654343, 0.00423447648063302, 0.035138633102178574, -0.009702893905341625, -0.09386512637138367, 0.0928555577993393, 0.008004925213754177, -0.00770544121041894, -0.05208677053451538, -0.012587991543114185, 0.003266935469582677, 0.006013551261276007, 0.007581597194075584, 0.01051716785877943, -0.08634556829929352, -0.06987878680229187, -0.002533907536417246, -0.09097649902105331, 0.046887341886758804, 0.052076555788517, 0.007193804252892733, 0.010903613641858101, -0.005229524336755276, 0.013937273994088173, 0.021968383342027664, 0.03420863673090935, 0.06022472307085991, 0.00011663129407679662, 0.0147319370880723, -0.07008928060531616, 0.02849903330206871, -0.02760171703994274, 0.010768436826765537, 0.03483092784881592, -0.022487863898277283, 0.009769013151526451, 0.07722778618335724, 0.02158835344016552, 0.11495622247457504, -0.06800112128257751, 0.023760998621582985, -0.015983954071998596, -0.017826983705163002, 0.06439491361379623, 0.032025717198848724, 0.050270259380340576, -0.005913740489631891, -0.03370802477002144, 0.017840281128883362, 0.01657339744269848, 0.0632966086268425, 0.034677211195230484, 0.04647349566221237, 0.09790609776973724, -0.006635513622313738, 0.025207048282027245, -0.07798825949430466, 0.016926418989896774, -0.0009457779233343899, 0.022471917793154716, -0.03825320303440094, 0.09570479393005371, -0.005350787658244371, 0.010469076223671436, -0.11524051427841187, -0.013262561522424221, -0.010709395632147789, -0.08311726897954941, 0.07327356934547424, 0.04939223825931549, -0.00899433996528387, -0.09584550559520721, 3.366148929092564e-33, 0.12493181973695755, 0.01934974081814289, -0.05822570249438286, -0.03598824888467789, -0.050746724009513855, -0.04566236957907677, -0.08260342478752136, 0.14819477498531342, -0.08842118829488754, 0.06027447059750557, 0.05103014037013054, 0.010303172282874584, 0.14121423661708832, 0.030813811346888542, 0.06103315204381943, -0.052851274609565735, 0.13664892315864563, 0.009189884178340435, -0.01732519268989563, -0.012848627753555775, -0.00799529068171978, -0.05098007991909981, -0.05235067382454872, 0.007593018934130669, -0.015166306868195534, 0.01696031726896763, 0.021270545199513435, 0.020558075979351997, -0.12002813071012497, 0.01446185540407896, 0.026759909465909004, 0.025330649688839912, -0.04275461286306381, 0.006768523249775171, -0.01445859856903553, 0.04526195675134659, -0.09147650748491287, -0.019439177587628365, -0.017833411693572998, -0.054910145699977875, -0.05264107510447502, -0.010459030978381634, -0.05201607942581177, 0.02089197374880314, -0.07997031509876251, -0.012111341580748558, -0.05773143470287323, 0.023178255185484886, -0.008031717501580715, -0.02598929964005947, -0.07995675504207611, -0.02072884514927864, 0.04881766065955162, -0.020389163866639137, -0.04917655140161514, 0.014159641228616238, -0.06362203508615494, -0.007807410322129726, 0.016431529074907303, -0.025682512670755386, 0.013381082564592361, 0.026248749345541, 0.009978425689041615, 0.06322886049747467, 0.0026721833273768425, -0.006582764443010092, 0.016631918027997017, 0.03236647695302963, 0.037942491471767426, -0.03637608885765076, -0.00691094808280468, 0.00015967199578881264, -0.0016335744876414537, -0.027278205379843712, -0.028038082644343376, 0.049681469798088074, -0.028867201879620552, -0.0024180635809898376, 0.014774911105632782, 0.009764522314071655, 0.0057976427488029, 0.013486180454492569, 0.005567898042500019, 0.03722712770104408, 0.007232568226754665, 0.0401562824845314, 0.08150329440832138, 0.0719916895031929, -0.013056193478405476, -0.042882055044174194, -0.011011265218257904, 0.004897848702967167, -0.009229736402630806, 0.03519150987267494, -0.05103505402803421, -1.571437735492509e-08, -0.08862441778182983, 0.02390933223068714, -0.016238776966929436, 0.0317004919052124, 0.02728421799838543, 0.05246885120868683, -0.047070927917957306, -0.05884743481874466, -0.06320816278457642, 0.04088856652379036, 0.049827951937913895, 0.10655169188976288, -0.07450232654809952, -0.01249542087316513, 0.01837068982422352, 0.039474111050367355, -0.024797888472676277, 0.014516275376081467, -0.037069227546453476, 0.020015744492411613, -4.8564306780463085e-05, 0.009866592474281788, 0.024838760495185852, -0.052458155900239944, 0.0293141920119524, -0.08719189465045929, -0.014499715529382229, 0.026019079610705376, -0.018746377900242805, -0.07620511204004288, 0.03504331782460213, 0.10363952070474625, -0.028050491586327553, 0.01271819043904543, -0.07632547616958618, -0.018652357161045074, 0.024976696819067, 0.08144534379243851, 0.06875885277986526, -0.06405666470527649, -0.08389385789632797, 0.0613623782992363, -0.033545542508363724, -0.10615336894989014, -0.04008054360747337, 0.03253024443984032, 0.07662481814622879, -0.07301618903875351, 0.00033757087658159435, -0.040871601551771164, -0.0757884755730629, 0.0275277066975832, 0.0746254101395607, 0.01771729439496994, 0.0912184864282608, 0.11022017151117325, 0.0005698017776012421, 0.051463354378938675, -0.01455135177820921, 0.03323202580213547, 0.02379227988421917, -0.022889869287610054, 0.0389375165104866, 0.030206838622689247]\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#//ignore warning\n",
    "\n",
    "print(embeddings)\n",
    "\n",
    "text = \"Hello world\"\n",
    "query_result = embeddings.embed_query(text) # Convert to vector\n",
    "print(query_result)\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "314301d2-92cf-4323-bea4-e6b416999b43",
   "metadata": {},
   "source": [
    "In Python Programming\n",
    "-------------------------\n",
    "|->module - existing pythoncode (file1.py)  => import file1\n",
    "file1.py                                      file:  p1.py\n",
    "----------                                    --------------\n",
    "def f1():                                       import file1\n",
    "    print(\"OK\")                                 var=200\n",
    "var=120                                         print(var) ->200\n",
    "print(var) # __main__.var ->120                 print(file1.var) ->120\n",
    "----------                                      f1() ->Error\n",
    "|                                               file1.f1() ->OK\n",
    "symbol table - dict - key:value                -----------------\n",
    "|                                               |__  __main__.var | 200\n",
    "__main__.f1 | 0x123                                  file1.var    | 120\n",
    "__main__.var| 120                                    file1.f1     | 0x123\n",
    "\n",
    "from <module> import <member>\n",
    "file: p2.py\n",
    "------------\n",
    "from file1 import f1\n",
    "from file1 import var\n",
    "f1() # ->OK \n",
    "print(var) ->120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008869a-760a-4206-ae5b-5ee10b64c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt template\n",
    "------------------\n",
    " |-> prompt - langchain tool\n",
    "    |->interact with LLM\n",
    "    |->interface between taking an input from user and structring a sentence that goes to LLM\n",
    "\n",
    "Input prompt (ex: what is the captial city of india?)\n",
    " |\n",
    "Retrieval system # data loader (ex: wikipedia,webportal,docs,...)\n",
    " |               # a document from Wikipedia ........ \"the captial city of india is delhi\"\n",
    "Augumentation Module\n",
    " # combines the input prompt with the retrieved docs ->augument prompt\n",
    " |\n",
    "LLM\n",
    " |\n",
    "Generate Answer based on augumentation module -> delhi\n",
    "\n",
    "PromptTemplate {input_variable = [\"label\"],template = \"message {label}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b27851-f6bb-417e-bb8e-7cab75ddbf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "096faf4a-8067-4092-89ea-af780c218069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "248f6358-12ec-4412-9219-3fcc6d9459e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read java book'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "prompt_obj.format(myvar=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1568531b-7dfc-4268-896c-298161fd459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj.format(myvar=\"GenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a24a868-ba57-4036-958b-c882da2b3d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['author', 'lng', 'topic'], input_types={}, partial_variables={}, template='Explain some short story about {topic} in {lng} written by {author}')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('Explain some short story about {topic} in {lng} written by {author}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c16bb1b-564c-4cc7-b7ec-d215575bc808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain some short story about birds in english written by Mr.abc'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pT=PromptTemplate.from_template('Explain some short story about {topic} in {lng} written by {author}')\n",
    "pT.format(topic=\"birds\",lng=\"english\",author=\"Mr.abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b977fda-f933-492c-9ac5-cc897ad5babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40a0902d-ee9a-49b1-b98f-bddca9003120",
   "metadata": {},
   "source": [
    "system - llm model -> ex: Hi your act as AI Er.\n",
    "user (or) human ->user will provide input. // {input_label}\n",
    "|\n",
    "chain = prompt|llm\n",
    "chain.invoke({'input':'Question'})\n",
    "                 |------------------------>{input}\n",
    "response=chain.invoke({'input':'Question'})\n",
    "|\n",
    "llm response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c0302fc-d405-4332-924e-9600fa60fcb1",
   "metadata": {},
   "source": [
    "#help(ChatPromptTemplate.from_messages)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25202b80-31a5-4178-b1f6-06ee8493ade1",
   "metadata": {},
   "source": [
    "ChatPromptTemplate.from_messages([(\"system\",\"baseline msg\"),(\"user\",\"user Input\")])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b0738fe-a934-417e-a19a-69104fd03571",
   "metadata": {},
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"system\",\"baseline msg\"),(\"user\",\"user Input\")])\n",
    "\n",
    "chain = prompt|llm\n",
    "              //combine this prompt template along with my llm => llm_prompt_chain\n",
    "chain.invoke(\"User Query\") ->response from llm (llm follows the promptemplate_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c57ffe6-769e-4839-b222-1d90241070b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8bf6529-d423-4bb8-acaa-1059f032301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bab1445a-b099-443a-8512-97787c8ed7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmit Tracking\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3456fa6-d72f-40b2-b2e1-100b7e577247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Your are a helpful AI assistant\"),(\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fd18cce-ff73-4825-947a-6650f732f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theeba\\AppData\\Local\\Temp\\ipykernel_23104\\2792764720.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma:2b\")\n"
     ]
    }
   ],
   "source": [
    "## Ollama model\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "chain = prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dae1cfa-c904-488f-a127-41f57731a6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, I can help you understand the term \"genAI\". Here\\'s a breakdown:\\n\\n**GenAI** is a relatively new term that refers to the intersection of **generative AI** and **artificial intelligence**. It encompasses the development of AI systems that can generate new content, such as text, images, music, and code, based on existing data.\\n\\nGenAI systems operate in a unique way compared to traditional AI. Instead of relying solely on supervised learning, they use various techniques like **reinforcement learning** and **generative adversarial networks** to generate new content. These techniques enable them to explore different possibilities and discover patterns in the data they\\'re trained on.\\n\\n**Here are some key characteristics of genAI:**\\n\\n* **Generative:** They create new content from scratch, unlike traditional AI models that rely on existing data.\\n* **Multimodal:** They can generate content in different formats, including text, images, audio, and code.\\n* **Adaptive:** They can learn and adapt to new data, enabling them to generate fresh content over time.\\n* **Unpredictable:** They often produce content that is difficult for humans to distinguish from real-world examples.\\n\\n**Here are some examples of genAI systems:**\\n\\n* **Chatbots:** Many chatbots use genAI to generate realistic conversations.\\n* **Language models:** Some large language models, like GPT, are considered genAI due to their ability to generate human-quality text.\\n* **Music generators:** GenAI is used by artists to create new music and sound effects.\\n* **Content creation platforms:** Tools like Midjourney and DALL-E are based on genAI to generate images and illustrations.\\n\\n**Overall, genAI is a rapidly evolving field with the potential to revolutionize how we create and interact with AI technology.**'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"what is genAI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a02dee0-229d-4132-ba8a-5304786b148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a step-by-step guide on how to write a factorial program in Python:\\n\\n**Step 1: Define a function to calculate factorial**\\n\\n```python\\ndef factorial(n):\\n    \"\"\"\\n    Calculates the factorial of a non-negative integer n.\\n\\n    Args:\\n        n (int): The factorial of n.\\n\\n    Returns:\\n        int: The factorial of n.\\n    \"\"\"\\n```\\n\\n**Step 2: Check for negative inputs and return an error**\\n\\n```python\\ndef factorial(n):\\n    if n < 0:\\n        return \"Error: Factorial is not defined for negative numbers.\"\\n```\\n\\n**Step 3: Calculate the factorial using a loop**\\n\\n```python\\ndef factorial(n):\\n    factorial_value = 1\\n    for i in range(1, n + 1):\\n        factorial_value *= i\\n    return factorial_value\\n```\\n\\n**Step 4: Print the result**\\n\\n```python\\nprint(f\"The factorial of {n} is {factorial(n)}\")\\n```\\n\\n**Example Usage:**\\n\\n```python\\n# Call the factorial function with different values of n\\nfactorial(5)\\nfactorial(-5)\\n```\\n\\n**Output:**\\n\\n```\\nThe factorial of 5 is 120\\nError: Factorial is not defined for negative numbers.\\n```\\n\\n**Additional Notes:**\\n\\n* The `factorial` function can be used to calculate the factorial of a non-negative integer.\\n* The loop implementation of factorial calculation can be replaced with a mathematical formula for n >= 2.\\n* You can use this function in various scenarios, such as finding the maximum factorial value for a given range of numbers.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"How to write factorial program in python?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51ee34b8-d90f-4bf8-bd35-4c4b5de2f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am unable to access external sources or the context of your question, so I cannot answer this question.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"what is nodeA?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b61de9f1-4ca5-43ae-a855-1eaac3fa189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get std output format\n",
    "# -------------------------\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20873b21-130b-4a2a-bd3b-1484f64ad387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help! \n",
      "\n",
      "**GenAI** is a **generative AI model** that focuses on generating human-quality text, code, and other forms of creative content. It is developed by Google AI.\n",
      "\n",
      "Here are some key characteristics of GenAI:\n",
      "\n",
      "* **Generative:** It can create new content based on existing data.\n",
      "* **Multimodal:** It can generate content in multiple formats, including text, code, images, and music.\n",
      "* **High-quality:** It produces highly realistic and coherent content.\n",
      "* **Large dataset:** It is trained on a massive dataset of text and code.\n",
      "* **Customizable:** It can be fine-tuned to generate specific types of content.\n",
      "\n",
      "**How to use GenAI:**\n",
      "\n",
      "* Go to the Google AI website (google.com/research/genai).\n",
      "* Click on \"Try GenAI.\"\n",
      "* Enter a prompt or a sample text, and GenAI will generate a sample output.\n",
      "* You can also use the following commands to control GenAI:\n",
      "    * `-s` or `--summary` to see a brief summary of the generated content.\n",
      "    * `-t` or `--text` to generate the content in a specific format (e.g., text, code, image).\n",
      "    * `--code` to generate the code equivalent of the generated text.\n",
      "\n",
      "**Examples of what GenAI can do:**\n",
      "\n",
      "* Write a story.\n",
      "* Create a song.\n",
      "* Translate a language.\n",
      "* Summarize a text.\n",
      "* Generate code for a specific task.\n"
     ]
    }
   ],
   "source": [
    "# prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Your are a helpful AI assistant\"),(\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "## Ollama model\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "output = StrOutputParser()\n",
    "chain = prompt|llm|output\n",
    "\n",
    "response = chain.invoke({'question':'what is genAI?'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83e33d3d-1107-4fe7-8c83-2bde4102b74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's how to write a factorial program in Python:\n",
      "\n",
      "```python\n",
      "# Define the factorial function\n",
      "def factorial(n):\n",
      "    # Initialize the factorial to 1\n",
      "    factorial = 1\n",
      "\n",
      "    # Iterate from 2 to n\n",
      "    for i in range(2, n + 1):\n",
      "        # Multiply the factorial of i by the factorial of n\n",
      "        factorial *= i\n",
      "\n",
      "    # Return the factorial\n",
      "    return factorial\n",
      "\n",
      "\n",
      "# Get the input from the user\n",
      "n = int(input(\"Enter a non-negative integer: \"))\n",
      "\n",
      "# Calculate and print the factorial\n",
      "result = factorial(n)\n",
      "print(f\"The factorial of {n} is {result}\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. The `factorial` function takes an integer `n` as its input.\n",
      "2. It initializes the `factorial` variable to 1, as the factorial of 0 is defined as 1.\n",
      "3. It then iterates from 2 to `n` (inclusive) using a `for` loop.\n",
      "4. Inside the loop, it multiplies the `factorial` of `i` by the `factorial` of `n`.\n",
      "5. Finally, it returns the calculated factorial.\n",
      "6. The user is prompted to input a non-negative integer, and the program calculates and prints the factorial of that number.\n",
      "\n",
      "**How to Use:**\n",
      "\n",
      "1. Save the code in a file named `factorial.py`.\n",
      "2. Run the script using the command: `python factorial.py`.\n",
      "3. Enter a non-negative integer as the input.\n",
      "4. The program will display the factorial of that number.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "```\n",
      "Enter a non-negative integer: 5\n",
      "The factorial of 5 is 120\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print((prompt|llm|output).invoke({'question':'how to write factorial program in python?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf20eee1-dc0b-41f7-8705-b70e2cb96334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm|output\n",
    "response = chain.invoke({'question':'how to write factorial program in python?'})\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41709f-2572-4fe2-8353-928cdaacd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompt engineering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e21cbf2-eaff-4706-ba4c-003e9a0deec2",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1st -> define  the prompt\n",
    "my_prompt = \"How to write hello world python program?\"\n",
    "\n",
    "# initialize a model and tokenizer \n",
    "model_name = \n",
    "my_response = pipeline('text-generation',model=model_name)\n",
    "\n",
    "# Generate text\n",
    "output = my_response(my_prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "506ddaf1-4eb0-4ad2-a5ea-c55052119905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import streamlit as st\n",
    "\n",
    "# Langsmit Tracking\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "\n",
    "# prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Your are a helpful AI assistant\"),(\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Ollama model\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "output = StrOutputParser()\n",
    "chain = prompt|llm|output\n",
    "\n",
    "# streamlit frame\n",
    "st.title(\"Langchain Demo with Gemma2 model\")\n",
    "input_text = st.text_input(\"Enter your query?\")\n",
    "\n",
    "if input_text:\n",
    "    st.write(chain.invoke({'question':input_text}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6278c142-caf1-4567-a62c-d961de6f9da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Groq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08de6edf-996d-4098-b97b-7a7bbf76f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "groqK = os.getenv('GROQ_API_KEY') #os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47aa8a6-4f78-47fa-8c77-f7c285910ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_groq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2fa374b8-754f-4c7f-b95a-2f1b8ac050a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "835c61da-5a9a-4b47-b138-f0f63714eae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000251DF97CA40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000251DF852360>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatGroq(model='gemma2-9b-it',groq_api_key=groqK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f80696fb-8337-462f-9d0d-8498829f7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model='gemma2-9b-it',groq_api_key=groqK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9cd24b1-24a1-49bf-871a-8bff10ff8a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Karthik, it's nice to meet you! \\n\\nIs there anything I can help you with today as an instructor? Perhaps you'd like to:\\n\\n* **Brainstorm lesson ideas?**\\n* **Get help with creating engaging activities?**\\n* **Find resources for your students?**\\n* **Practice your lesson delivery?**\\n\\n\\nLet me know how I can be of service. üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 18, 'total_tokens': 108, 'completion_time': 0.163636364, 'prompt_time': 9.387e-05, 'queue_time': 0.01927827, 'total_time': 0.163730234}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-1e9ff4eb-1d15-448b-96da-a9a369687843-0', usage_metadata={'input_tokens': 18, 'output_tokens': 90, 'total_tokens': 108})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hello My name is karthik and im instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9af32681-5b0b-4fd9-8936-7d3cf375e1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" üëã\\n\\nHow can I help you today?\\n\\nAre you looking to:\\n\\n* **Generate creative content?** (e.g., stories, poems, articles)\\n* **Answer your questions?** (I have access to a vast knowledge base)\\n* **Translate languages?**\\n* **Summarize text?**\\n* **Code in different programming languages?**\\n* **Or something else entirely?**\\n\\n\\nLet me know what's on your mind! üí°\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 21, 'total_tokens': 124, 'completion_time': 0.187272727, 'prompt_time': 0.000139889, 'queue_time': 0.019823681, 'total_time': 0.187412616}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-92ba14a7-1582-4d98-a654-232e19026dfd-0', usage_metadata={'input_tokens': 21, 'output_tokens': 103, 'total_tokens': 124})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([AIMessage(content=\"Hello Karthik, it's nice to meet you!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1fd6548c-1fd0-4744-8200-0f9e511fb3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' doing today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 21, 'total_tokens': 27, 'completion_time': 0.010909091, 'prompt_time': 0.00014413, 'queue_time': 0.02163998, 'total_time': 0.011053221}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a854a4c6-839f-42a6-ab8c-27a4849570dd-0', usage_metadata={'input_tokens': 21, 'output_tokens': 6, 'total_tokens': 27})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi,My name is Tom\"),\n",
    "        AIMessage(content=\"Hello Tom, How are you\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b004506-2953-4b6d-8eb6-5e5144f48a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hey there! üëã  \\n\\nWhat can I do for you today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 32, 'total_tokens': 49, 'completion_time': 0.030909091, 'prompt_time': 0.000314819, 'queue_time': 0.020685901, 'total_time': 0.03122391}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-032b3e58-f778-461b-ba07-e8e5f578e515-0', usage_metadata={'input_tokens': 32, 'output_tokens': 17, 'total_tokens': 49})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi,My name is Tom\"),\n",
    "        AIMessage(content=\"Hello Tom, How are you\"),\n",
    "        HumanMessage(content='Hey')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f65208da-4598-439b-b4f0-54dcdaf1c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1f75c2ca-8af6-4d4d-8460-1957720897fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id:str) ->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c96095f9-c164-4b1b-b76b-816ba6eb5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(RunnableWithMessageHistory)\n",
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f0745e5-38da-49c1-b83d-5a76678096c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Karthik, it's nice to meet you! üëã \\n\\nWhat can I do for you today? üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 16, 'total_tokens': 43, 'completion_time': 0.049090909, 'prompt_time': 9.838e-05, 'queue_time': 0.02072368, 'total_time': 0.049189289}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-988d7637-a1d2-47f5-981d-71f5f5be228e-0', usage_metadata={'input_tokens': 16, 'output_tokens': 27, 'total_tokens': 43})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content=\"Hi,My name is karthik\")],config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91520759-8e7a-4199-975b-9cb489174ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke([HumanMessage(content=\"Hi,My name is karthik\")],config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eae1d023-a70c-4d03-bf78-9f9eb4b3433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Karthik.  You told me at the beginning of our conversation! üòÑ  \\n\\nIs there anything else I can help you with? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 56, 'total_tokens': 90, 'completion_time': 0.061818182, 'prompt_time': 0.001899374, 'queue_time': 0.021775206000000002, 'total_time': 0.063717556}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f25b47f3-0aab-4018-9b62-8dbc3674f1d1-0', usage_metadata={'input_tokens': 56, 'output_tokens': 34, 'total_tokens': 90})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4d3ac3d9-f9f1-4732-b725-1f56565c3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chatA\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b30b41e4-8eee-48b4-bc2f-5bb784e7afab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to use it! üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 14, 'total_tokens': 54, 'completion_time': 0.072727273, 'prompt_time': 8.557e-05, 'queue_time': 0.0210032, 'total_time': 0.072812843}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-16eb41b5-38d7-4f92-aed2-c2e47f3e52a3-0', usage_metadata={'input_tokens': 14, 'output_tokens': 40, 'total_tokens': 54})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9bbe7301-2c48-4d73-b166-f8f4fd538a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0c6313ec-a1a2-4f32-8525-639596363659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Karthik. üòä \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 103, 'total_tokens': 125, 'completion_time': 0.04, 'prompt_time': 0.009622108, 'queue_time': 0.031710061, 'total_time': 0.049622108}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f9967212-7003-4679-bb1d-3b6ba21c3ea3-0', usage_metadata={'input_tokens': 103, 'output_tokens': 22, 'total_tokens': 125})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "14e35bf5-704f-436b-81df-8152c770b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "### prompt template\n",
    "\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\",\"you are helpful AI assistant\"),\n",
    "                                 MessagesPlaceholder(variable_name=\"message\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c18ca8e5-57c8-497e-8dfd-1537248c1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "27f47a9e-5bcd-45cb-a839-26d97e2798b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Tom! It's nice to meet you. \\n\\nWhat can I do to help you today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 20, 'total_tokens': 48, 'completion_time': 0.050909091, 'prompt_time': 8.33e-05, 'queue_time': 0.020781959, 'total_time': 0.050992391}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-27aa401e-f965-451a-afd2-60293d742c5a-0', usage_metadata={'input_tokens': 20, 'output_tokens': 28, 'total_tokens': 48})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"message\":[HumanMessage(content=\"Hi,My name is Tom\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4a62ad22-b538-41d0-a284-0c1a0e608510",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "45e8edd8-646b-48c9-8c3e-ce14c8a7a75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a helpful AI assistant, I have no memory of past conversations and no access to your personal information. So I don't know your name!\\n\\nPlease tell me your name, and I'll be happy to use it. üòä  \\n\\n\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chatB\"}}\n",
    "response=with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397ea79-37a8-450c-a8c7-74d1520d8a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc457f-607a-4c0b-8b18-e24925cd03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## language translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e68f234c-463c-4f7c-a261-169b1eb147c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\"You are AI assist,Answer all the questions in {language}.\"),\n",
    "     MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81da5aa7-d9da-4a92-aaee-2860fb274251",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "84a454d3-ef9d-4f8e-b7f3-3ebdc74f895d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§≤‡•Ä‡§ì! \\n\\n‡§Ü‡§™‡§ï‡•á ‡§∏‡§µ‡§æ‡§≤‡•ã‡§Ç ‡§ï‡•á ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡•Ç‡§Ç‡§ó‡§æ‡•§  \\n\\n‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? üòä\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9dfcef4f-8116-4417-ac0d-a0c19e7bba2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç, Leo! \\n\\n‡Æé‡Æ©‡Øç‡Æ© ‡Æµ‡Æø‡Æ©‡Ææ‡Æï‡Øç‡Æï‡Æ≥‡Øç? \\n\\n‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ§‡Æµ‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. üòä \\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"Tamil\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9e31fc48-e1e5-4d1c-8bff-cca1544a1a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡≤®‡≤Æ‡≥ç‡≤Æ‡≥Ü‡≤≤‡≥ç‡≤≤‡≤∞‡≥Ç ‡≤®‡≤Æ‡≤∏‡≥ç‡≤ï‡≤æ‡≤∞, ‡≤≤‡≤ø‡≤Ø‡≥ä! üòä  \\n\\n‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤â‡≤§‡≥ç‡≤§‡≤∞‡≤ø‡≤∏‡≤≤‡≥Å ‡≤§‡≤Ø‡≤æ‡≤∞‡≤ø‡≤¶‡≤ø‡≤¶‡≥ç‡≤¶‡≥á‡≤®‡≥Ü. \\n\\n‡≤®‡≥Ä‡≤µ‡≥Å ‡≤è‡≤®‡≥Å ‡≤ï‡≥á‡≤≥‡≤¨‡≤Ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≥Ä‡≤∞‡≤ø? \\n\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"kannada\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4bdcde97-48b7-4880-a0e6-c748ca8c262e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞æ‡∞ï‡±Å ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å, ‡∞≤‡±Ä‡∞Ø‡±ã ‡∞Ö‡∞®‡±á‡∞¶‡∞ø ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å. üòä \\n\\n‡∞è ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡±Å ‡∞Ö‡∞°‡∞ø‡∞ó‡∞ø‡∞§‡±á ‡∞®‡±á‡∞®‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞ö‡±Ü‡∞¨‡±Å‡∞§‡∞æ‡∞®‡±Å.  \\n\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"telugu\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "faf883a0-3c01-47d0-bf6d-92cd94b5de81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour Leo, \\n\\nEnchant√© de te rencontrer ! Comment puis-je t'aider aujourd'hui ? \\n\\n\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"french\"})\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
