import os
from dotenv import load_dotenv
load_dotenv()
-----
# Langsmit Tracking
os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')
os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')
os.environ['LANGCHAIN_TRACING_V2']="true"

----
from langchain_groq import ChatGroq
gK = os.getenv('GROQ_API_KEY')
----
llm = ChatGroq(model='Llama3-8b-8192',groq_api_key=gK)
llm
----
!pip install langchain_chroma
----
from langchain_community.vectorstores import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
----

Webbase loader
|
Day2 - webbase loader ->bs4 
	|
	docs = loader.load()
	|
	text_Splitter=RecursiveCharacterTextSplitter
	|
	chunks=text_Splitter.split_documents(docs)
	|
	vectorstore=Chroma.from_documents(documents=chunks,embeddig=<>)
	
--
os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')
from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
embeddings
---
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate 
-------------
import bs4

url='https://lilianweng.github.io/posts/2023-06-23-agent/'
loader=WebBaseLoader(web_path=(url),bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=("post-title","post-content","post-header")),))
docs = loader.load()
docs
---------------------
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)
splits = text_splitter.split_documents(docs)
-----------------------
vectorstore = Chroma.from_documents(documents=splits,embedding=embeddings)

retriver = vectorstore.as_retriever()
retriever
------------------------
## Prompt Template

prompt = ChatPromptTemplate.from_messages([
    (
        "system","you are AI assist\n{context}"
    ),
    (
        "human","{input}"
    )
])
----------------------------
qa_chain = create_stuff_documents_chain(llm,prompt)
rag_chain = create_retrieval_chain(retriver,qa_chain)
-----------------------------
response = rag_chain.invoke({"input":"what is Self-Reflection"})
#print(response)
print(response['answer'])
-------------------------------
response = rag_chain.invoke({"input":"How do we move fast"})
print(response['answer'])
--------------------------------


Summarization
---------------
loader = PyPDFLoader("LVMActivity.pdf")
docs = loader.load_and_split()

chunk_prompt="""
Please Summarize the below text Content: {text} 
Summary:"""

map_prompt_template = PromptTemplate(input_variables=['text'],template=chunk_prompt)

final_prompt=""" Provide final Summary of the entire pdf Content: {text}"""

final_prompt_template = PromptTemplate(input_variables=['text'],template=final_prompt)

summary_chain = load_summarize_chain(llm=llm,chain_type='map_reduce',
                                     map_prompt=map_prompt_template,
                                     combine_prompt=final_prompt_template,
                                     verbose=True)
summary_chain.run(docs)
---------------------------------------------------------------------------------------
