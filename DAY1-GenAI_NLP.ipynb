{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d81d9-3c32-43f5-99cb-8e1b1617f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb8b4fe-9bc3-4903-bcca-2a4a09e59f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a06ba4c0-634c-4d01-baef-8ff463e5ede2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello welcome,to OU NLP training.',\n",
       " 'Please do activity the entire couse module!',\n",
       " \"to become expert in NLP.ab'c defg\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=\"\"\"Hello welcome,to OU NLP training.\n",
    "Please do activity the entire couse module! to become expert in NLP.ab'c defg\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#help(sent_tokenize)\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f789aa7-6c77-4a45-8784-23b91a294a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'OU',\n",
       " 'NLP',\n",
       " 'training',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'activity',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'couse',\n",
       " 'module',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP.ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'defg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e46c214a-22d0-473b-8c24-1f3bd15472e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "305ab776-ad20-483b-a389-ceb58225f6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'OU',\n",
       " 'NLP',\n",
       " 'training',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'activity',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'couse',\n",
       " 'module',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'defg']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b78d4eb0-b7b3-4165-a1a1-4d1de4171843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer\n",
    "\n",
    "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55b6d3df-28da-4c24-9b43-af8a072012e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44f63d8a-96be-4e62-8172-cc2a43b93f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'abc.ABCMeta'>\n"
     ]
    }
   ],
   "source": [
    "print(type(PorterStemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae7df2cd-3dc2-40d7-bc28-16efdb47d619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "stemming.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "344de80a-99a6-458d-b28a-6d935b72a686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "'''each and every word will apply stemming word'''\n",
    "\n",
    "for var in words:\n",
    "    print(var+\"----->\"+stemming.stem(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e9bf7aa-7c32-4464-baf5-b570869ebe50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87b607b7-7c3c-4d19-aeae-a028c767b894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regular Expression\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "#help(RegexpStemmer)\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$',min=4)\n",
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "960b584e-e0d7-4b25-82fb-4886c22f2dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87618056-0c79-44d1-8f44-137e2bbe0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stem = SnowballStemmer('english')\n",
    "#help(SnowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "851d19ac-3dda-4876-8acc-c13ee94431a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "'''each and every word will apply stemming word'''\n",
    "\n",
    "for var in words:\n",
    "    print(var+\"----->\"+snowball_stem.stem(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa7cb2ec-0603-43b2-a671-adb37a17cfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'fair')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Portstemmer Vs Snowballstemmer\n",
    "stemming.stem(\"fairly\"),snowball_stem.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6177887-1284-4d3b-8fb1-21c8b92da535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b491fda8-5a01-4400-a536-564c4c3f2ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2bf8391-75fa-4e63-8ae1-b9d6f380571d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "015a518b-3ca8-4bd0-b16e-5bb44bc0c292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('going',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c24d8341-8cf8-45f1-8a1c-ee9b2afd713c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eating',\n",
       " 'eats',\n",
       " 'eaten',\n",
       " 'writing',\n",
       " 'writes',\n",
       " 'programming',\n",
       " 'programs',\n",
       " 'history',\n",
       " 'finally',\n",
       " 'finalized']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1afd462a-4140-4e94-a6cf-ca86a430858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->history\n",
      "finally---->finally\n",
      "finalized---->finalize\n"
     ]
    }
   ],
   "source": [
    "for var in words:\n",
    "    print(var+\"---->\"+lemmatizer.lemmatize(var,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2057ec8c-459d-445e-9672-687f39436928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "409109e8-344c-4f91-8f1f-745dca6109f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\theeba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5bcd770b-d1f1-406c-a686-c337148a119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9b88ae6-1510-42ec-8645-db37895e315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa7195-d9f3-4232-9803-df457e6dc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Take corpus\n",
    "|\n",
    "apply stop words -> set() \n",
    "|\n",
    "snowball steamming\n",
    "|\n",
    "lemmatization\n",
    "|\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d563f26b-fa56-4d60-a61a-96ea48eeacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Vikram Ambalal Sarabhai was one of the greatest scientists of India. He is widely regarded as the father of the Indian space programme. In fact he was a rare combination of a scientist, an innovator, industrialist and a visionary.\n",
    "\n",
    "Vikram Sarabhai was born on August 12, 1919 at Ahmedabad in an affluent family of progressive industrialists. He was one of the eight children of Ambalal and Sarla Devi. He had his early education in a private school, 'Retreat', run by his parents on Montessori lines. Some of the great personalities like Gurudev Rabindranath, J Krishna Murthi, Motilal Nehru, VS Shrinivasa Shastri, Jawaharlal Nehru, Sarojini Naidu, Maulana Azad, CF Andrews, C V Raman et al used to stay with the Sarabhai family when they visited Ahmedabad. Mahatma Gandhi also once stayed at their house while recovering from an illness. Visits by such great personalities greatly influenced Vikram Sarabhai.\n",
    "\n",
    "After his matriculation, Vikram Sarabhai proceeded to Cambridge for his college education and took the tripos in Natural Sciences from St. John's College in 1940. With the beginning of World War II, he returned home and joined as a research scholar under Sir CV Raman at the Indian Institute of Science, Bangalore. His interest in solar physics and cosmic ray led him to set up many observation stations around the country. He built the necessary equipment and took measurements at Bangalore, Pune and the Himalayas. He returned to Cambridge in 1945 and completed his PhD in 1947.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e6ef4e7-894f-4c59-a166-b782c57b242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Copus ->documents\n",
    "sentences = nltk.sent_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c2f82fc-6a81-4c87-821c-2c8fa90f684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 14\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences),len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3042d410-887b-43e9-bec3-a63729ef8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "054fb55f-245b-40a5-ab6a-6eba4a8f0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords ->snowball stemming\n",
    "for v in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[v])\n",
    "    words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[v]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01ceb855-9a90-407a-a260-e034294bd518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vikram ambal sarabhai one greatest scientist india .',\n",
       " 'he wide regard father indian space programm .',\n",
       " 'in fact rare combin scientist , innov , industrialist visionari .',\n",
       " 'vikram sarabhai born august 12 , 1919 ahmedabad affluent famili progress industrialist .',\n",
       " 'he one eight children ambal sarla devi .',\n",
       " \"he earli educ privat school , retreat ' , run parent montessori line .\",\n",
       " 'some great person like gurudev rabindranath , j krishna murthi , motil nehru , vs shrinivasa shastri , jawaharl nehru , sarojini naidu , maulana azad , cf andrew , c v raman et al use stay sarabhai famili visit ahmedabad .',\n",
       " 'mahatma gandhi also stay hous recov ill .',\n",
       " 'visit great person great influenc vikram sarabhai .',\n",
       " \"after matricul , vikram sarabhai proceed cambridg colleg educ took tripo natur scienc st. john 's colleg 1940 .\",\n",
       " 'with begin world war ii , return home join research scholar sir cv raman indian institut scienc , bangalor .',\n",
       " 'his interest solar physic cosmic ray led set mani observ station around countri .',\n",
       " 'he built necessari equip took measur bangalor , pune himalaya .',\n",
       " 'he return cambridg 1945 complet phd 1947 .']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "98e5617e-41a5-409b-8fa4-e3c536e49c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data1', 'Data2', 'data1', 'data2', 'data3'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=[\"Data1\",\"Data2\",\"data3\",\"data1\",\"data2\",\"Data2\"]\n",
    "set(L) # type cast to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1d049f29-4dcc-4aba-9150-71ae7979805e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 101, 102, 103, 104]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=[]\n",
    "for var in range(5):\n",
    "    r = var + 100\n",
    "    L.append(r)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eee2a22c-5e23-4521-8fc7-a85819366ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 101, 102, 103, 104]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var+100 for var in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595b17e-d535-4c83-9977-7110e29785b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task - lemitization\n",
    "#\n",
    "# -> create lemitization object\n",
    "# -> stem ->lema //convert ->lowercase   Ab AB ab -> ab ab ab ->ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5fa32fe7-52fe-4989-a1f7-bc0a3588948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for var in range(len(sentences)):\n",
    "    sentences[v]=sentences[v].lower()\n",
    "    words = nltk.word_tokenize(sentences[v])\n",
    "    words = [lemmatizer.lemmatize(word,pos='v')for word in words]\n",
    "    sentences[v]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e3537442-38af-4641-b764-60351e15c506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vikram ambal sarabhai one greatest scientist india .',\n",
       " 'he wide regard father indian space programm .',\n",
       " 'in fact rare combin scientist , innov , industrialist visionari .',\n",
       " 'vikram sarabhai born august 12 , 1919 ahmedabad affluent famili progress industrialist .',\n",
       " 'he one eight children ambal sarla devi .',\n",
       " \"he earli educ privat school , retreat ' , run parent montessori line .\",\n",
       " 'some great person like gurudev rabindranath , j krishna murthi , motil nehru , vs shrinivasa shastri , jawaharl nehru , sarojini naidu , maulana azad , cf andrew , c v raman et al use stay sarabhai famili visit ahmedabad .',\n",
       " 'mahatma gandhi also stay hous recov ill .',\n",
       " 'visit great person great influenc vikram sarabhai .',\n",
       " \"after matricul , vikram sarabhai proceed cambridg colleg educ took tripo natur scienc st. john 's colleg 1940 .\",\n",
       " 'with begin world war ii , return home join research scholar sir cv raman indian institut scienc , bangalor .',\n",
       " 'his interest solar physic cosmic ray led set mani observ station around countri .',\n",
       " 'he built necessari equip took measur bangalor , pune himalaya .',\n",
       " 'return cambridg 1945 complet phd 1947 .']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a9724637-3d10-46b2-bccf-3086100216dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vikram', 'NN'), ('ambal', 'JJ'), ('sarabhai', 'NN'), ('one', 'CD'), ('greatest', 'JJS'), ('scientist', 'NN'), ('india', 'NN'), ('.', '.')]\n",
      "[('he', 'PRP'), ('wide', 'JJ'), ('regard', 'NN'), ('father', 'NN'), ('indian', 'JJ'), ('space', 'NN'), ('programm', 'NN'), ('.', '.')]\n",
      "[('in', 'IN'), ('fact', 'NN'), ('rare', 'JJ'), ('combin', 'NN'), ('scientist', 'NN'), (',', ','), ('innov', 'NN'), (',', ','), ('industrialist', 'NN'), ('visionari', 'NN'), ('.', '.')]\n",
      "[('vikram', 'NN'), ('sarabhai', 'NN'), ('born', 'VBN'), ('august', 'RB'), ('12', 'CD'), (',', ','), ('1919', 'CD'), ('ahmedabad', 'NN'), ('affluent', 'JJ'), ('famili', 'NN'), ('progress', 'NN'), ('industrialist', 'NN'), ('.', '.')]\n",
      "[('he', 'PRP'), ('one', 'CD'), ('eight', 'CD'), ('children', 'NNS'), ('ambal', 'JJ'), ('sarla', 'NN'), ('devi', 'NN'), ('.', '.')]\n",
      "[('he', 'PRP'), ('earli', 'VBZ'), ('educ', 'JJ'), ('privat', 'JJ'), ('school', 'NN'), (',', ','), ('retreat', 'NN'), (\"'\", \"''\"), (',', ','), ('run', 'VB'), ('parent', 'NN'), ('montessori', 'FW'), ('line', 'NN'), ('.', '.')]\n",
      "[('some', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('like', 'IN'), ('gurudev', 'NN'), ('rabindranath', 'NN'), (',', ','), ('j', 'NN'), ('krishna', 'NN'), ('murthi', 'NN'), (',', ','), ('motil', 'NN'), ('nehru', 'NN'), (',', ','), ('vs', 'FW'), ('shrinivasa', 'NN'), ('shastri', 'NN'), (',', ','), ('jawaharl', 'NN'), ('nehru', 'NN'), (',', ','), ('sarojini', 'NN'), ('naidu', 'NN'), (',', ','), ('maulana', 'NN'), ('azad', 'NN'), (',', ','), ('cf', 'NN'), ('andrew', 'NN'), (',', ','), ('c', 'VBP'), ('v', 'JJ'), ('raman', 'NN'), ('et', 'NN'), ('al', 'NN'), ('use', 'NN'), ('stay', 'NN'), ('sarabhai', 'JJ'), ('famili', 'JJ'), ('visit', 'NN'), ('ahmedabad', 'NN'), ('.', '.')]\n",
      "[('mahatma', 'NN'), ('gandhi', 'NN'), ('also', 'RB'), ('stay', 'VBP'), ('hous', 'JJ'), ('recov', 'NN'), ('ill', 'NN'), ('.', '.')]\n",
      "[('visit', 'NN'), ('great', 'JJ'), ('person', 'NN'), ('great', 'JJ'), ('influenc', 'JJ'), ('vikram', 'NN'), ('sarabhai', 'NN'), ('.', '.')]\n",
      "[('after', 'IN'), ('matricul', 'NN'), (',', ','), ('vikram', 'FW'), ('sarabhai', 'NNS'), ('proceed', 'VBP'), ('cambridg', 'JJ'), ('colleg', 'NN'), ('educ', 'NN'), ('took', 'VBD'), ('tripo', 'NN'), ('natur', 'NN'), ('scienc', 'JJ'), ('st.', 'NN'), ('john', 'NN'), (\"'s\", 'POS'), ('colleg', 'NN'), ('1940', 'CD'), ('.', '.')]\n",
      "[('with', 'IN'), ('begin', 'JJ'), ('world', 'NN'), ('war', 'NN'), ('ii', 'NN'), (',', ','), ('return', 'VBP'), ('home', 'NN'), ('join', 'NN'), ('research', 'NN'), ('scholar', 'NN'), ('sir', 'VBD'), ('cv', 'JJ'), ('raman', 'NN'), ('indian', 'JJ'), ('institut', 'NN'), ('scienc', 'NN'), (',', ','), ('bangalor', 'NN'), ('.', '.')]\n",
      "[('his', 'PRP$'), ('interest', 'NN'), ('solar', 'NN'), ('physic', 'JJ'), ('cosmic', 'JJ'), ('ray', 'NN'), ('led', 'VBD'), ('set', 'VBN'), ('mani', 'NN'), ('observ', 'JJ'), ('station', 'NN'), ('around', 'IN'), ('countri', 'NN'), ('.', '.')]\n",
      "[('he', 'PRP'), ('built', 'VBD'), ('necessari', 'JJ'), ('equip', 'NN'), ('took', 'VBD'), ('measur', 'NN'), ('bangalor', 'NN'), (',', ','), ('pune', 'JJ'), ('himalaya', 'NN'), ('.', '.')]\n",
      "[('return', 'NN'), ('cambridg', 'NN'), ('1945', 'CD'), ('complet', 'NN'), ('phd', 'NN'), ('1947', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for var in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[var])\n",
    "    words = [word for word in words]\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "68edbc4b-f998-484b-ace0-f5f8ce6ed798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pos_tag in module nltk.tag:\n",
      "\n",
      "pos_tag(tokens, tagset=None, lang='eng')\n",
      "    Use NLTK's currently recommended part of speech tagger to\n",
      "    tag the given list of tokens.\n",
      "\n",
      "        >>> from nltk.tag import pos_tag\n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")) # doctest: +NORMALIZE_WHITESPACE\n",
      "        [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),\n",
      "        (\"n't\", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"), tagset='universal') # doctest: +NORMALIZE_WHITESPACE\n",
      "        [('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),\n",
      "        (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n",
      "\n",
      "    NB. Use `pos_tag_sents()` for efficient tagging of more than one sentence.\n",
      "\n",
      "    :param tokens: Sequence of tokens to be tagged\n",
      "    :type tokens: list(str)\n",
      "    :param tagset: the tagset to be used, e.g. universal, wsj, brown\n",
      "    :type tagset: str\n",
      "    :param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian\n",
      "    :type lang: str\n",
      "    :return: The tagged tokens\n",
      "    :rtype: list(tuple(str, str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cc18cecf-a57b-4156-80fc-700f6bcce490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vikram', 'NN'), ('ambal', 'JJ'), ('sarabhai', 'NN'), ('one', 'CD'), ('greatest', 'JJS'), ('scientist', 'NN'), ('india', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "s1=\"vikram ambal sarabhai one greatest scientist india\"\n",
    "words = nltk.word_tokenize(s1)\n",
    "tag_elements = nltk.pos_tag(words)\n",
    "print(tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9be9e035-a117-42f0-bf92-b52a3ca475d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "805f6a09-a1f4-4f6c-886e-a9ce414568db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "voc = ['the','food','good','bad','ok']\n",
    "label_obj = LabelEncoder()\n",
    "label_encoded = label_obj.fit_transform(voc)\n",
    "\n",
    "\n",
    "obj1 = OneHotEncoder(sparse_output=False) # (sparse=False)\n",
    "label_encoded = label_encoded.reshape(len(label_encoded),1)\n",
    "\n",
    "obj1.fit_transform(label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "474cb9f4-9093-4547-aeec-7c507053db63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the',), ('food',), ('is',), ('good',)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "s = \"the food is good\"\n",
    "\n",
    "words = word_tokenize(s)\n",
    "list(ngrams(words,1)) # <or> we can use for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5a0ed7cc-215d-4278-aded-d66672411372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object f1 at 0x000002084E143270>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generator \n",
    "def f1():\n",
    "    yield \"D1\"\n",
    "    yield \"F1\",\"F2\"\n",
    "    yield \"L1\",\"L2\",\"L3\"\n",
    "f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fba75b00-8a92-4cb4-9abb-c0a6aed1e25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D1', ('F1', 'F2'), ('L1', 'L2', 'L3')]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = f1()\n",
    "list(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4546e20c-7efa-4b68-924d-e4f8dafbab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1\n",
      "('F1', 'F2')\n",
      "('L1', 'L2', 'L3')\n"
     ]
    }
   ],
   "source": [
    "obj =f1()\n",
    "for var in obj:\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cf249d63-e874-4f54-9c77-b96dd52c3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the',), ('food',), ('is',), ('good',)]\n",
      "[('the', 'food'), ('food', 'is'), ('is', 'good')]\n",
      "[('the', 'food', 'is'), ('food', 'is', 'good')]\n",
      "[('the', 'food', 'is', 'good')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "s = \"the food is good\"\n",
    "\n",
    "words = word_tokenize(s)\n",
    "print(list(ngrams(words,1))) # \n",
    "print(list(ngrams(words,2))) # bigrams\n",
    "print(list(ngrams(words,3))) # trigrams\n",
    "print(list(ngrams(words,4))) # higherorder\n",
    "print(list(ngrams(words,5))) # higherorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7f002647-ea5b-4dec-9aec-52190efc7381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1():\n",
    "    return \"the\"\n",
    "\n",
    "f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f2b29da0-d18c-459e-ab06-df745eec9d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the',)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1():\n",
    "    return \"the\", # more than one return value ->tuple\n",
    "\n",
    "f1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
